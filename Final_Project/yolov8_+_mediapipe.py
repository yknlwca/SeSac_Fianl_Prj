# -*- coding: utf-8 -*-
"""YOLOv8 + MediaPipe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OCO3Gp_aOAMbQ7gv75k4uICIdzmvNt0D
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install ultralytics

!pip install mediapipe

# Commented out IPython magic to ensure Python compatibility.
import cv2
import mediapipe as mp
import numpy as np
import os
import random
import gc
import time
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt

from IPython.display import Image, clear_output
from torch.cuda import memory_allocated, empty_cache
from torch.optim import Adam
from torch.utils.data import random_split
from torch.utils.data import Dataset, DataLoader
from glob import glob
from tqdm import tqdm
from google.colab.patches import cv2_imshow
from ultralytics import YOLO
import torch
import torchvision

# %matplotlib inline

yolo_model = YOLO('yolov8s.pt')

# 디바이스 설정
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
yolo_model = yolo_model.to(device)

BATCH_SIZE = 6
EPOCH = 100
NUM_LAYERS = 1      # LSTM model: num_layers
start_dot = 11      # mp.solutions.pose 시작 포인트 (0: 얼굴부터 발목까지, 11: 어깨부터 발목까지)
n_CONFIDENCE = 0.3    # MediaPipe Min Detectin confidence check

mp_pose = mp.solutions.pose
attention_dot = [n for n in range(start_dot, 29)]

# 라인 그리기
if start_dot == 11:
# 몸 부분만 머리는 신경 안써도 될듯?
    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \
                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \
                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \
                [23, 24], [12, 24]]
    print('Pose : Only Body')

else:
# 몸 전체
    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \
                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \
                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \
                [23, 24], [12, 24], [9, 10], [0, 5], [0, 2], [5, 8], [2, 7]]
    print('Pose : Face + Body')

def get_skeleton(video_path, attention_dot, draw_line):
    frame_length = 30
    xy_list_list, xy_list_list_flip = [], []
    pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=False, min_detection_confidence=n_CONFIDENCE)
    cap = cv2.VideoCapture(video_path)

    if cap.isOpened():
        while True:
            ret, img = cap.read()
            if not ret:
                break

            img = cv2.resize(img, (640, 640))
            results = yolo_model(img)

            # YOLO 모델 출력 처리
            for result in results:
                for box in result.boxes:
                    xyxy = box.xyxy[0].cpu().numpy()
                    confidence = box.conf[0]
                    xx1, yy1, xx2, yy2 = xyxy[:4]

                    if confidence > y_CONFIDENCE:
                        c_img = img[int(yy1):int(yy2), int(xx1):int(xx2)]
                        results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB))

                    if not results.pose_landmarks:
                        continue

                    idx = 0
                    xy_list, xy_list_flip = [], []
                    for x_and_y in results.pose_landmarks.landmark:
                        if idx in attention_dot:
                            xy_list.append(x_and_y.x)
                            xy_list.append(x_and_y.y)
                            xy_list_flip.append(1 - x_and_y.x)
                            xy_list_flip.append(x_and_y.y)

                            x, y = int(x_and_y.x * (xx2 - xx1)), int(x_and_y.y * (yy2 - yy1))
                            xy_list_list.append(xy_list)
                            xy_list_list_flip.append(xy_list_flip)
                        idx += 1



        if len(xy_list_list_flip) < frame_length:
            f_ln = frame_length - len(xy_list_list_flip)
            for _ in range(f_ln):
                xy_list_list.append(xy_list_list[-1])
                xy_list_list_flip.append(xy_list_list_flip[-1])

    cap.release()
    cv2.destroyAllWindows()
    return xy_list_list, xy_list_list_flip

# 영상 데이터에서 mp pose landmark dot 데이터 추출

video_path = '/content/drive/MyDrive/CCTV/Data'
raw_data = []

for fold in os.listdir(video_path):
    for video_name in os.listdir(os.path.join(video_path, fold)):
        if int(video_name.split('_')[3][:2]) >= 30:
            label = 0 if 'normal' in video_name else 1
            skel_data_n, skel_data_f = get_skeleton(os.path.join(video_path, fold, video_name), attention_dot, draw_line)
            if skel_data_n is not None and len(skel_data_n) > 0:
                seq_list_n = skel_data_n[:30]
                seq_list_f = skel_data_f[:30]
                raw_data.append({'key': label, 'value': seq_list_n})
                raw_data.append({'key': label, 'value': seq_list_f})

random.shuffle(raw_data)

# 길이
nd = 0
ad = 0
for i in range(len(raw_data)):
    if raw_data[i]['key'] == 0:
        nd += 1
    else:
        ad += 1
print('normal data:', nd, '| abnormal data:', ad)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

class MyDataset(Dataset):
    def __init__(self, seq_list):
        self.X = []
        self.y = []
        for dic in seq_list :
            self.y.append(dic['key'])
            self.X.append(dic['value'])

    def __getitem__(self, index):
        data = self.X[index]
        label = self.y[index]
        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))

    def __len__(self):
        return len(self.X)

split_ratio = [0.8, 0.1, 0.1]
train_len = int(len(raw_data) * split_ratio[0])
val_len = int(len(raw_data) * split_ratio[1])
test_len = len(raw_data) - train_len - val_len

print('{}, {}, {}'.format(train_len, val_len, test_len))

train_dataset = MyDataset(raw_data)
train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)
val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)
test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)

# LSTM
class skeleton_LSTM(nn.Module):
    def __init__(self):
        super(skeleton_LSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size=len(attention_dot) * 2, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=NUM_LAYERS, batch_first=True)
        self.dropout1 = nn.Dropout(0.1)
        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)
        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)
        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=NUM_LAYERS, batch_first=True)
        self.dropout2 = nn.Dropout(0.1)
        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=NUM_LAYERS, batch_first=True)
        self.fc = nn.Linear(32,2)

    def forward(self, x) :
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, _ = self.lstm3(x)
        x = self.dropout1(x)
        x, _ = self.lstm4(x)
        x, _ = self.lstm5(x)
        x, _ = self.lstm6(x)
        x = self.dropout2(x)
        x, _ = self.lstm7(x)
        x = self.fc(x[:,-1,:]) # x[배치 크기, 시퀀스 길이, 은닉 상태 크기], [:, -1, :] -> 마지막 시간 단계만 선택

        return x

# 모델 초기화

def init_model():
    global net, loss_fn, optim
    plt.rc('font', size = 10)
    net = skeleton_LSTM().to(device)
    loss_fn = nn.CrossEntropyLoss()
    optim = Adam(net.parameters(), lr=0.0001)

# epoch 카운터 초기화
def init_epoch():
    global epoch_cnt
    epoch_cnt = 0

# 모든 Log를 초기화
def init_log():
    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log
    plt.rc('font', size = 10)
    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []

def record_train_log(_tloss, _tacc, _time):
    # Train Log 기록
    time_log.append(_time)
    tloss_log.append(_tloss)
    tacc_log.append(_tacc)
    iter_log.append(epoch_cnt)


def record_valid_log(_vloss, _vacc):
    # Validation Log 기록
    vloss_log.append(_vloss)
    vacc_log.append(_vacc)

def last(log_list):
    # last 안의 마지막 숫자를 반환(print_log 함수에서 사용)
    if len(log_list) > 0:
        return log_list[len(log_list) - 1]
    else:
        return -1

def print_log():
    # 학습 추이 출력 : 소숫점 3자리까지
    train_loss = round(float(last(tloss_log)), 3)
    train_acc = round(float(last(tacc_log)), 3)
    val_loss = round(float(last(vloss_log)), 3)
    val_acc = round(float(last(vacc_log)), 3)
    time_spent = round(float(last(time_log)), 3)

    log_str = 'Epoch: {:3} | T_Loss {:5} | T_Acc {:5} | V_Loss {:5} | V_Acc {:5} | {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)

    log_stack.append(log_str)

    # 학습 추이 그래프 출력
    hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)
    hist_fig.patch.set_facecolor('white')

    # Loss Line 구성
    loss_t_line = plt.plot(iter_log, tloss_log, label='Train_Loss', color='red', marker='o')
    loss_v_line = plt.plot(iter_log, vloss_log, label='Valid_Loss', color='blue', marker='s')
    loss_axis.set_xlabel('epoch')
    loss_axis.set_ylabel('loss')

    # Acc, Line 구성
    acc_axis = loss_axis.twinx()
    acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train_Acc', color='red', marker='+')
    acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid_Acc', color='blue', marker='x')
    acc_axis.set_ylabel('accuracy')

    # 그래프 출력
    hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line
    loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])
    loss_axis.grid()
    plt.title('Learning history until epoch {}'.format(last(iter_log)))
    plt.draw()

    # 텍스트 로그 출력
    clear_output(wait=True)
    plt.show()
    for idx in reversed(range(len(log_stack))):
        print(log_stack[idx])

def clear_memory():
    if device != 'cpu':
        empty_cache()
    gc.collect()

# 학습 알고리즘
def epoch(data_loader, mode = 'train'):
    global epoch_cnt

    # 사용되는 변수 초기화
    iter_loss, iter_acc, last_grad_performed = [], [], False

    # 1 iteration 학습 알고리즘(for문을 나오면 1 epoch 완료)
    for _data, _label in data_loader:
        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)

        # 1. Feed-forward
        if mode == 'train':
            net.train()
        else:
            # 학습때만 쓰이는 Dropout, Batch Mormalization을 미사용
            net.eval()

        result = net(data) # 1 Batch에 대한 결과가 모든 Class에 대한 확률값으로
        _, out = torch.max(result, 1) # result에서 최대 확률값을 기준으로 예측 class 도출( _ : 값 부분은 필요 없음, out : index 중 가장 큰 하나의 데이터)

        # 2. Loss 계산
        loss = loss_fn(result, label) # GT 와 Label 비교하여 Loss 산정
        iter_loss.append(loss.item()) # 학습 추이를 위하여 Loss를 기록

        # 3. 역전파 학습 후 Gradient Descent
        if mode == 'train':
            optim.zero_grad() # 미분을 통해 얻은 기울기를 초기화 for 다음 epoch
            loss.backward() # 역전파 학습
            optim.step() # Gradient Descent 수행
            last_grad_performed = True # for문을 나가면 epoch 카운터 += 1

        # 4. 정확도 계산
        acc_partial = (out == label).float().sum() # GT == Label 인 개수
        acc_partial = acc_partial / len(label) # ( TP / (TP + TM)) 해서 정확도 산출
        iter_acc.append(acc_partial.item()) # 학습 추이를 위하여 Acc. 기록

    # 역전파 학습 후 Epoch 카운터 += 1
    if last_grad_performed:
        epoch_cnt += 1

    clear_memory()

    # loss와 acc의 평균값 for 학습추이 그래프, 모든 GT와 Label 값 for 컨퓨전 매트릭스
    return np.average(iter_loss), np.average(iter_acc)

def epoch_not_finished():
    # 에폭이 끝남을 알림
    return epoch_cnt < maximum_epoch

# Training initialization
init_model()
init_epoch()
init_log()
maximum_epoch = EPOCH

# Training iteration

while epoch_not_finished():
    start_time = time.time()

    tloss, tacc = epoch(train_loader, mode = 'train')

    end_time = time.time()
    time_taken = end_time - start_time
    record_train_log(tloss, tacc, time_taken)

    with torch.no_grad():
        vloss, vacc = epoch(val_loader, mode = 'val')
        record_valid_log(vloss, vacc)

    print_log()

print('\n 학습 완료!')

# 정확도 검증
with torch.no_grad():
    test_loss, test_acc = epoch(test_loader, mode = 'test')
    test_acc = round(test_acc, 4)
    test_loss = round(test_loss, 4)
    print('Test Acc.: {}'.format(test_acc))
    print('Test Loss: {}'.format(test_loss))

# 영상 resize 및 추출
test_video_name = 'C_3_12_10_BU_DYA_07-27_13-01-22_CA_RGB_DF2_F2'
test_video_path = f'/content/drive/MyDrive/CCTV/Abnormal/Abnormal/{test_video_name}.mp4'
cv2.destroyAllWindows()
cap = cv2.VideoCapture(test_video_path)
img_list = []

if cap.isOpened():

    while True:
        ret, img = cap.read()
        if ret:
            img = cv2.resize(img, (640, 640))
            img_list.append(img)
            # cv2_imshow(img)
            # cv2.waitKey(1)
        else:
            break

cap.release()
cv2.destroyAllWindows()

print('저장된 frame의 개수: {}'.format(len(img_list)))

def process_detections(res, img, xy_list_list):
    img_height, img_width = img.shape[:2]
    for box in res.boxes:
        # 경계 상자 좌표 추출 및 검증
        xyxy = box.xyxy.cpu().numpy()[0]  # 첫 번째 객체에 대한 좌표 추출
        confidence = box.conf.cpu().numpy()[0]  # 첫 번째 객체에 대한 신뢰도 추출
        xx1, yy1, xx2, yy2 = xyxy[:4]

        if confidence > y_CONFIDENCE:
            # 경계 상자 좌표 조정 및 Mediapipe 처리
            xx1 = max(0, int(xx1) - 10)
            yy1 = max(0, int(yy1) - 10)
            xx2 = min(img_width - 1, int(xx2) + 10)
            yy2 = min(img_height - 1, int(yy2) + 10)

            c_img = img[yy1:yy2, xx1:xx2]
            results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB))
            if not results.pose_landmarks: continue

            xy_list = [x_and_y.x for idx, x_and_y in enumerate(results.pose_landmarks.landmark) if idx in attention_dot] + \
                      [x_and_y.y for idx, x_and_y in enumerate(results.pose_landmarks.landmark) if idx in attention_dot]
            xy_list_list.append(xy_list)


# 신경망 모델 평가 모드 설정
net.eval()
length = 30  # frame 상태를 표시할 길이
out_img_list = []
status = 'None'
print('시퀀스 데이터 분석 중...')

# 이미지 리스트 처리
xy_list_list = []
for img in tqdm(img_list):
    res = yolo_model(img)

    # YOLO 모델 출력 처리
    if isinstance(res, list):
        # 여러 결과가 리스트로 반환될 경우, 각각 처리
        for r in res:
            process_detections(r, img, xy_list_list)
    else:
        # 단일 결과 처리
        process_detections(res, img, xy_list_list)

    # 지정된 길이의 데이터가 쌓이면 모델에 입력
    if len(xy_list_list) == length:
        data_tensor = torch.tensor([xy_list_list], dtype=torch.float32).to(device)
        with torch.no_grad():
            result = net(data_tensor)
            _, out = torch.max(result, 1)
            status = 'Normal' if out.item() == 0 else 'Abnormal'
        xy_list_list = []  # 리스트 초기화

    # 결과 이미지에 상태 표시
    cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)
    out_img_list.append(img)

# 테스트 원본 영상 내보내기
filename = '/content/drive/MyDrive/CCTV/output.mp4'
fourcc = cv2.VideoWriter_fourcc(*'DIVX')
fps = 3
frameSize = (640, 640)
isColor = True
out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)
for out_img in out_img_list:
    out.write(out_img)
out.release()

# 모델 저장하기
PATH = '/content/drive/MyDrive/CCTV/'
model_name = 'LSTM_2nd.pt'
torch.save(net.state_dict(), PATH + model_name)