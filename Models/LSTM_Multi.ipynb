{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1NF0EcQ07wrDskCuJ9fvAD-nMQi5R4re7","authorship_tag":"ABX9TyN0LyOaMaN0q6z1VJUYU93v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install mediapipe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9n7suV2F1ApT","executionInfo":{"status":"ok","timestamp":1703220105387,"user_tz":-540,"elapsed":7381,"user":{"displayName":"KwangLim Choi","userId":"12810006642826653647"}},"outputId":"193b2168-8be7-4a38-b9b9-aa85042bebec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n","Installing collected packages: sounddevice, mediapipe\n","Successfully installed mediapipe-0.10.9 sounddevice-0.4.6\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BCngKy_402FB","executionInfo":{"status":"ok","timestamp":1703228728486,"user_tz":-540,"elapsed":11166,"user":{"displayName":"KwangLim Choi","userId":"12810006642826653647"}}},"outputs":[],"source":["import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","import random\n","import gc\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","from IPython.display import Image, clear_output\n","from torch.cuda import memory_allocated, empty_cache\n","from torch.optim import Adam\n","from torch.utils.data import random_split\n","from torch.utils.data import Dataset, DataLoader\n","from glob import glob\n","from tqdm import tqdm\n","from google.colab.patches import cv2_imshow\n","\n","%matplotlib inline"]},{"cell_type":"code","source":["BATCH_SIZE = 6\n","EPOCH = 700\n","NUM_LAYERS = 1      # LSTM model: num_layers\n","start_dot = 11      # mp.solutions.pose 시작 포인트 (0: 얼굴부터 발목까지, 11: 어깨부터 발목까지)\n","n_CONFIDENCE = 0.3    # MediaPipe Min Detectin confidence check\n","\n","mp_pose = mp.solutions.pose\n","attention_dot = [n for n in range(start_dot, 29)]\n","\n","# 라인 그리기\n","if start_dot == 11:\n","    \"\"\"몸 부분만\"\"\"\n","    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n","                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n","                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n","                [23, 24], [12, 24]]\n","    print('Pose : Only Body')\n","\n","else:\n","    \"\"\"얼굴 포함\"\"\"\n","    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n","                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n","                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n","                [23, 24], [12, 24], [9, 10], [0, 5], [0, 2], [5, 8], [2, 7]]\n","    print('Pose : Face + Body')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Evs14my50_qS","executionInfo":{"status":"ok","timestamp":1703228728486,"user_tz":-540,"elapsed":45,"user":{"displayName":"KwangLim Choi","userId":"12810006642826653647"}},"outputId":"b7d0b885-1f71-48ec-aa40-bd541f7d122b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Pose : Only Body\n"]}]},{"cell_type":"code","source":["# media pipe landmark 데이터 전처리 함수\n","\n","def get_skeleton(video_path, attention_dot, draw_line):\n","    frame_length = 30 # LSTM 모델에 넣을 frame 수\n","    xy_list_list, xy_list_list_flip = [], []\n","    cv2.destroyAllWindows()\n","    pose = mp_pose.Pose(static_image_mode = True, model_complexity = 1, \\\n","                        enable_segmentation = False, min_detection_confidence = n_CONFIDENCE)\n","    cap = cv2.VideoCapture(video_path)\n","\n","    if cap.isOpened():\n","        while True:\n","            ret, img = cap.read()\n","            if ret == True:\n","                xy_list, xy_list_flip = [], []\n","                img = cv2.resize(img, (640, 640))\n","                results = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","                if not results.pose_landmarks: continue # landmark 못잡는 경우도 종종 발생\n","                idx = 0\n","                draw_line_dic = {}\n","                # 33 반복문 진행 : 33개 중 18개의 dot\n","                for x_and_y in results.pose_landmarks.landmark:\n","                    if idx in attention_dot:\n","                        xy_list.append(x_and_y.x)\n","                        xy_list.append(x_and_y.y)\n","                        xy_list_flip.append(1 - x_and_y.x)\n","                        xy_list_flip.append(x_and_y.y)\n","                        x, y = int(x_and_y.x*640), int(x_and_y.y*640)\n","                        draw_line_dic[idx] = [x, y]\n","                    idx += 1\n","                if len(xy_list) != len(attention_dot) * 2:\n","                    print('Error : attention_dot 데이터 오류')\n","\n","                xy_list_list.append(xy_list)\n","                xy_list_list_flip.append(xy_list_flip)\n","\n","                \"\"\"mediapipe line 그리기 부분 : 데이터 추출(dot) 확인용\"\"\"\n","                # for line in draw_line:\n","                #     x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n","                #     x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n","                #     img = cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 4)\n","                # # cv2.imshow('Landmark Image', img)\n","                # cv2_imshow(img)\n","                # cv2.waitKey(1)\n","\n","            elif ret == False: break\n","\n","        # 부족한 프레임 수 맞추기\n","        if len(xy_list_list_flip) < frame_length:\n","            f_ln = frame_length - len(xy_list_list_flip)\n","            for _ in range(f_ln):\n","                xy_list_list.append(xy_list_list[-1])\n","                xy_list_list_flip.append(xy_list_list_flip[-1])\n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","\n","    return xy_list_list, xy_list_list_flip"],"metadata":{"id":"2fd7WK811GzJ","executionInfo":{"status":"ok","timestamp":1703228728487,"user_tz":-540,"elapsed":8,"user":{"displayName":"KwangLim Choi","userId":"12810006642826653647"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["video_path = '/content/drive/MyDrive/CCTV/clips' # dataset 경로\n","video_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"jFROiKtf1WRR","executionInfo":{"status":"ok","timestamp":1703228728487,"user_tz":-540,"elapsed":7,"user":{"displayName":"KwangLim Choi","userId":"12810006642826653647"}},"outputId":"f5df7339-df01-4e21-8020-e70274735528"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/CCTV/clips'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["raw_data = []\n","\n","for fold in os.listdir(video_path):\n","    for video_name in os.listdir(video_path + '/' + fold):\n","        if int(video_name.split('_')[3][:2]) >= 30:  # video name 참조\n","                # 레이블링 수정\n","            if video_name.split('_')[2] == 'normal': label = 0\n","            elif video_name.split('_')[2] == 'theft': label = 1\n","            elif video_name.split('_')[2] == 'broken': label = 2\n","\n","            video_file_path = os.path.join(video_path, video_name)\n","            skel_data_n, skel_data_f = get_skeleton(video_file_path, attention_dot, draw_line)\n","            if skel_data_n != False:\n","                seq_list_n = skel_data_n[:30]\n","                seq_list_f = skel_data_f[:30]\n","                raw_data.append({'key':label, 'value':seq_list_n})\n","                raw_data.append({'key':label, 'value':seq_list_f})\n","\n","random.shuffle(raw_data)"],"metadata":{"id":"O5PDoSg01y7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 카테고리별 데이터 개수 초기화\n","category_counts = {'normal': 0, 'theft': 0, 'violence': 0, 'broken': 0}\n","\n","for data in raw_data:\n","    label = data['key']\n","    if label == 0:\n","        category_counts['normal'] += 1\n","    elif label == 1:\n","        category_counts['theft'] += 1\n","    elif label == 2:\n","        category_counts['broken'] += 1\n","\n","# 결과 출력\n","print('Category Counts:')\n","for category, count in category_counts.items():\n","    print(f'{category}: {count}')"],"metadata":{"id":"EiFiZpEJ1y4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"eyj2yllC1y2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, seq_list):\n","        self.X = []\n","        self.y = []\n","        for dic in seq_list:\n","            self.y.append(dic['key'])\n","            self.X.append(dic['value'])\n","\n","    def __getitem__(self, index):\n","        data = self.X[index]\n","        label = self.y[index]\n","        return torch.Tensor(data), torch.tensor(label)  # torch.Tensor로 변환\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","\n","split_ratio = [0.8, 0.1, 0.1]\n","train_len = int(len(raw_data) * split_ratio[0])\n","val_len = int(len(raw_data) * split_ratio[1])\n","test_len = len(raw_data) - train_len - val_len\n","\n","print('{}, {}, {}'.format(train_len, val_len, test_len))"],"metadata":{"id":"WwP8jHXi1y0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = MyDataset(raw_data)\n","train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n","\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n","val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"],"metadata":{"id":"_awxz5Kk1yyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class skeleton_LSTM(nn.Module):\n","    def __init__(self):\n","        super(skeleton_LSTM, self).__init__()\n","        self.lstm1 = nn.LSTM(input_size=len(attention_dot) * 2, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n","        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n","        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=NUM_LAYERS, batch_first=True)\n","        self.dropout1 = nn.Dropout(0.1)\n","        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n","        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n","        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=NUM_LAYERS, batch_first=True)\n","        self.dropout2 = nn.Dropout(0.1)\n","        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=NUM_LAYERS, batch_first=True)\n","        self.fc = nn.Linear(32, 3)  # 변경된 부분: 출력 크기를 3으로 설정\n","\n","    def forward(self, x) :\n","        x, _ = self.lstm1(x)\n","        x, _ = self.lstm2(x)\n","        x, _ = self.lstm3(x)\n","        x = self.dropout1(x)\n","        x, _ = self.lstm4(x)\n","        x, _ = self.lstm5(x)\n","        x, _ = self.lstm6(x)\n","        x = self.dropout2(x)\n","        x, _ = self.lstm7(x)\n","        x = self.fc(x[:,-1,:])  # 마지막 시간 단계만 선택\n","\n","        return x"],"metadata":{"id":"GJuqUJ6z1ywC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_model():\n","    global net, loss_fn, optim\n","    plt.rc('font', size = 10)\n","    net = skeleton_LSTM().to(device)\n","    loss_fn = nn.CrossEntropyLoss()\n","    optim = Adam(net.parameters(), lr=0.0001)\n","\n","# epoch 카운터 초기화\n","def init_epoch():\n","    global epoch_cnt\n","    epoch_cnt = 0\n","\n","# 모든 Log를 초기화\n","def init_log():\n","    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log\n","    plt.rc('font', size = 10)\n","    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n","    time_log, log_stack = [], []\n","\n","def init_early_stopping():\n","    global best_val_loss, patience_counter\n","    best_val_loss = float('inf')  # 최소 손실을 무한대로 설정\n","    patience_counter = 0  # 카운터 초기화"],"metadata":{"id":"qSGSLtcg1yt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def record_train_log(_tloss, _tacc, _time):\n","    # Train Log 기록\n","    time_log.append(_time)\n","    tloss_log.append(_tloss)\n","    tacc_log.append(_tacc)\n","    iter_log.append(epoch_cnt)\n","\n","def record_valid_log(_vloss, _vacc):\n","    # Validation Log 기록\n","    vloss_log.append(_vloss)\n","    vacc_log.append(_vacc)\n","\n","def last(log_list):\n","    # last 안의 마지막 숫자를 반환(print_log 함수에서 사용)\n","    if len(log_list) > 0:\n","        return log_list[len(log_list) - 1]\n","    else:\n","        return -1\n","\n","def print_log():\n","    # 학습 추이 출력 : 소숫점 3자리까지\n","    train_loss = round(float(last(tloss_log)), 3)\n","    train_acc = round(float(last(tacc_log)), 3)\n","    val_loss = round(float(last(vloss_log)), 3)\n","    val_acc = round(float(last(vacc_log)), 3)\n","    time_spent = round(float(last(time_log)), 3)\n","\n","    log_str = 'Epoch: {:3} | T_Loss {:5} | T_Acc {:5} | V_Loss {:5} | V_Acc {:5} | {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)\n","\n","    log_stack.append(log_str)\n","\n","    # 학습 추이 그래프 출력\n","    hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)\n","    hist_fig.patch.set_facecolor('white')\n","\n","    # Loss Line 구성\n","    loss_t_line = plt.plot(iter_log, tloss_log, label='Train_Loss', color='red', marker='o')\n","    loss_v_line = plt.plot(iter_log, vloss_log, label='Valid_Loss', color='blue', marker='s')\n","    loss_axis.set_xlabel('epoch')\n","    loss_axis.set_ylabel('loss')\n","\n","    # Acc, Line 구성\n","    acc_axis = loss_axis.twinx()\n","    acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train_Acc', color='red', marker='+')\n","    acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid_Acc', color='blue', marker='x')\n","    acc_axis.set_ylabel('accuracy')\n","\n","    # 그래프 출력\n","    hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line\n","    loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])\n","    loss_axis.grid()\n","    plt.title('Learning history until epoch {}'.format(last(iter_log)))\n","    plt.draw()\n","\n","    # 텍스트 로그 출력\n","    clear_output(wait=True)\n","    plt.show()\n","    for idx in reversed(range(len(log_stack))):\n","        print(log_stack[idx])"],"metadata":{"id":"lbx0bBPC1yrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clear_memory():\n","    if device != 'cpu':\n","        empty_cache()\n","    gc.collect()\n","\n","# 학습 알고리즘\n","def epoch(data_loader, mode = 'train'):\n","    global epoch_cnt\n","\n","    # 사용되는 변수 초기화\n","    iter_loss, iter_acc, last_grad_performed = [], [], False\n","\n","    # 1 iteration 학습 알고리즘(for문을 나오면 1 epoch 완료)\n","    for _data, _label in data_loader:\n","        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)\n","\n","        # 1. Feed-forward\n","        if mode == 'train':\n","            net.train()\n","        else:\n","            # 학습때만 쓰이는 Dropout, Batch Mormalization을 미사용\n","            net.eval()\n","\n","        result = net(data) # 1 Batch에 대한 결과가 모든 Class에 대한 확률값으로\n","        _, out = torch.max(result, 1) # result에서 최대 확률값을 기준으로 예측 class 도출( _ : 값 부분은 필요 없음, out : index 중 가장 큰 하나의 데이터)\n","\n","        # 2. Loss 계산\n","        loss = loss_fn(result, label) # GT 와 Label 비교하여 Loss 산정\n","        iter_loss.append(loss.item()) # 학습 추이를 위하여 Loss를 기록\n","\n","        # 3. 역전파 학습 후 Gradient Descent\n","        if mode == 'train':\n","            optim.zero_grad() # 미분을 통해 얻은 기울기를 초기화 for 다음 epoch\n","            loss.backward() # 역전파 학습\n","            optim.step() # Gradient Descent 수행\n","            last_grad_performed = True # for문을 나가면 epoch 카운터 += 1\n","\n","        # 4. 정확도 계산\n","        acc_partial = (out == label).float().sum() # GT == Label 인 개수\n","        acc_partial = acc_partial / len(label) # ( TP / (TP + TM)) 해서 정확도 산출\n","        iter_acc.append(acc_partial.item()) # 학습 추이를 위하여 Acc. 기록\n","\n","    # 역전파 학습 후 Epoch 카운터 += 1\n","    if last_grad_performed:\n","        epoch_cnt += 1\n","\n","    clear_memory()\n","\n","    # loss와 acc의 평균값 for 학습추이 그래프, 모든 GT와 Label 값 for 컨퓨전 매트릭스\n","    return np.average(iter_loss), np.average(iter_acc)\n","\n","def epoch_not_finished():\n","    # 에폭이 끝남을 알림\n","    return epoch_cnt < maximum_epoch"],"metadata":{"id":"2uwGkXud1ypi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_model()\n","init_epoch()\n","init_log()\n","maximum_epoch = EPOCH\n","patience = 50  # 이 값은 필요에 따라 조정할 수 있습니다.\n","init_early_stopping()\n","\n","\n","# Training iteration\n","\n","while epoch_not_finished():\n","    start_time = time.time()\n","\n","    tloss, tacc = epoch(train_loader, mode = 'train')\n","\n","    end_time = time.time()\n","    time_taken = end_time - start_time\n","    record_train_log(tloss, tacc, time_taken)\n","\n","    with torch.no_grad():\n","        vloss, vacc = epoch(val_loader, mode = 'val')\n","        record_valid_log(vloss, vacc)\n","    if vloss < best_val_loss:\n","        best_val_loss = vloss\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","\n","    print_log()\n","\n","    if patience_counter >= patience:\n","        print(\"Early stopping triggered after {} epochs\".format(epoch_cnt))\n","        break\n","\n","print('\\n Training completed!')"],"metadata":{"id":"ZlGlMKSx1ynK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    test_loss, test_acc = epoch(test_loader, mode = 'test')\n","    test_acc = round(test_acc, 4)\n","    test_loss = round(test_loss, 4)\n","    print('Test Acc.: {}'.format(test_acc))\n","    print('Test Loss: {}'.format(test_loss))"],"metadata":{"id":"-gj7HQ6W1ylJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_video_name = 'C_3_12_44_BU_SMC_10-14_12-19-55_CE_RGB_DF2_F2'\n","test_video_path = f'/content/drive/MyDrive/Behavior/Abnormal/Theft_Video/{test_video_name}.mp4'\n","cv2.destroyAllWindows()\n","cap = cv2.VideoCapture(test_video_path)\n","img_list = []\n","\n","if cap.isOpened():\n","\n","    while True:\n","        ret, img = cap.read()\n","        if ret:\n","            img = cv2.resize(img, (640, 640))\n","            img_list.append(img)\n","            # cv2_imshow(img)\n","            # cv2.waitKey(1)\n","        else:\n","            break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","print('저장된 frame의 개수: {}'.format(len(img_list)))"],"metadata":{"id":"8juvMu6P1yih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 라인 그리기 및 상태 이상 표시하여 모델 테스트\n","net.eval()\n","\n","length = 30 # frame 상태를 표시할 길이\n","out_img_list = []\n","dataset = []\n","status = 'None'\n","status_labels = {0: 'Normal', 1: 'Theft', 2: 'Violence', 3: 'Broken'}\n","pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=False, min_detection_confidence=n_CONFIDENCE)\n","print('시퀀스 데이터 분석 중...')\n","\n","xy_list_list = []\n","for img in tqdm(img_list):\n","    results = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","    if not results.pose_landmarks: continue\n","    xy_list = []\n","    idx = 0\n","    draw_line_dic = {}\n","    for x_and_y in results.pose_landmarks.landmark:\n","        if idx in attention_dot:\n","            xy_list.append(x_and_y.x)\n","            xy_list.append(x_and_y.y)\n","            x, y = int(x_and_y.x * 640), int(x_and_y.y * 640)\n","            draw_line_dic[idx] = [x, y]\n","        idx += 1\n","\n","    xy_list_list.append(xy_list)\n","    for line in draw_line:\n","        x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n","        x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n","        img = cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 4)\n","\n","    if len(xy_list_list) == length:\n","        dataset = []\n","        dataset.append({'key' : 0, 'value' : xy_list_list})\n","        dataset = MyDataset(dataset)\n","        dataset = DataLoader(dataset, batch_size=1) # 배치 크기 지정\n","        xy_list_list = []\n","\n","        for data, label in dataset:\n","            data = data.to(device)\n","            with torch.no_grad():\n","                result = net(data)\n","                _, out = torch.max(result, 1)\n","                status = status_labels[out.item()]\n","                if status in ['Theft','Broken']:\n","                    font_scale = 2.0\n","                    font_color = (0, 0, 255)  # 빨간색\n","                    thickness = 2\n","                        # 경고 테두리 추가\n","                    cv2.rectangle(img, (0, 0), (img.shape[1], img.shape[0]), font_color, 10)\n","\n","                else:\n","                    font_scale = 1.0\n","                    font_color = (255, 0, 0)  # 파란색\n","                    thickness = 1\n","\n","        cv2.putText(img, status, (10, 50), cv2.FONT_HERSHEY_COMPLEX, font_scale, font_color, thickness)\n","    out_img_list.append(img)"],"metadata":{"id":"CAWPx8eq1ygh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = '/content/drive/MyDrive/CCTV/9___multi_7.mp4'\n","fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n","fps = 3\n","frameSize = (640, 640)\n","isColor = True\n","out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)\n","for out_img in out_img_list:\n","    out.write(out_img)\n","out.release()"],"metadata":{"id":"62mNwLNq1yeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HNQKaqZi1ycJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pMiIDCwi1yXh"},"execution_count":null,"outputs":[]}]}